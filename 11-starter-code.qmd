---
title: "11-starter-code"
format: docx
editor: visual
---

# Import our Libraries and Data

```{r}
# Load packages.
library(tidyverse)
library(tidymodels)

# Import soup data and select the Sales, Any_Disp_Spend, Any_Feat_Spend, and Any_Price_Decr_Spend variables

```

# For Illustrative Purposes ONLY!

We are adding random variables to our data to see how they affect our model. We DO NOT normally do this.

```{r}
set.seed(7)

soup_data <- soup_data |> 
  mutate(
    nonsense_01 = rnorm(nrow(soup_data), mean = 0, sd = 5),
    nonsense_02 = rnorm(nrow(soup_data), mean = 50, sd = 10),
    nonsense_03 = rnorm(nrow(soup_data), mean = -10, sd = 20),
    nonsense_04 = rnorm(nrow(soup_data), mean = 100, sd = 100),
    nonsense_05 = runif(nrow(soup_data), min = -5, max = 5),
    nonsense_06 = runif(nrow(soup_data), min = 25, max = 500),
    nonsense_07 = runif(nrow(soup_data), min = -50, max = 10),
    nonsense_08 = rbinom(nrow(soup_data), size = 1, prob = 0.25),
    nonsense_09 = rbinom(nrow(soup_data), size = 3, prob = 0.10),
    nonsense_10 = rbinom(nrow(soup_data), size = 7, prob = 0.85)
  )
```

# Data Splitting (We DO Normally Do This)

In order to compute predictive fit, we need to split the data into **training** data (used to fit the model) and **testing** data (used to compute predictive fit).

```{r}
# Set the randomization seed.
set.seed(42)

# Split the data.
soup_split <- initial_split(soup_data, prop = 0.90)

soup_split
```

`initial_split()` appends 'meta-information' for other functions to use (i.e., which observations are contained in the train and test data sets). Essentially, we are adding either a `Training` label or a `Testing` label to each observation in the original data.

We can access the training data with `training()`.

```{r}
# Access the training data.
training(soup_split)
```

We can access the testing data with `testing()`.

```{r}
# Access the testing data.
testing(soup_split)
```

## Fit with Training Data

We fit the model using only the *training data* to fit (i.e., train) the model.

```{r}
# Fit the simple model with training data.
fit_simple <- linear_reg() |> 
  set_engine("lm") |> 
  fit(
    Sales ~ Any_Disp_Spend + Any_Feat_Spend + Any_Price_Decr_Spend, 
    data = training(soup_split)
  )

# Fit the complex model with training data
fit_complex <- linear_reg() |> 
  set_engine("lm") |> 
  fit(
    Sales ~ ., 
    data = training(soup_split)
  )
```

## Assess Model Performance with R-Squared (Unreliable)

```{r}
# Compare overall model fit.
bind_rows(
  glance(fit_simple),
  glance(fit_complex)
)
```

Which model is *technically* better based on $R$-squared?

## Predict with Testing Data (THIS IS THE WAY.)

Now that we have trained our models, we use `predict()` for model comparison where the `new_data` is the testing data. We compare the predicted outcome to the *actual* outcome and compute the **root mean squared error** (RMSE), a squared average of the difference between the truth and our estimate. Closer to 0 is better!

```{r}
# Compute predictive fit for simple model
fit_simple |> 
  predict(new_data = testing(soup_split)) |>
  bind_cols(testing(soup_split)) |>
  rmse(truth = Sales, estimate = .pred)
```

```{r}
# compute predictive fit for complex model
fit_complex |> 
  predict(new_data = testing(soup_split)) |>
  bind_cols(testing(soup_split)) |>
  rmse(truth = Sales, estimate = .pred)
```

According to the test data RMSE, which model gives the best fit?

## What About Training Data Predictions? (Common Mistake)

While you can compute the prediction error on the training data, DO NOT choose your model based on which one performs best on the training data.

```{r}
# Compute predictive fit for the simple model on the training data (BAD)
fit_simple |> 
  predict(new_data = training(soup_split)) |>
  bind_cols(training(soup_split)) |>
  rmse(truth = Sales, estimate = .pred)
```

```{r}
# Compute predictive fit for the complex model on the training data (BAD)
fit_complex |> 
  predict(new_data = training(soup_split)) |>
  bind_cols(training(soup_split)) |>
  rmse(truth = Sales, estimate = .pred)
```

Based on training data predictions, which model is better/more accurate? Is this the expected result?

## Wait...What Am I Supposed to Do?

1. *Train* models on *Train* Data
2. *Choose* your best model after *Testing* it on *Test* Data

## Live Coding

`iFood` was happy with the work you completed on your projects. They have asked us to build a model for the total amount that a customer purchases (i.e., a proxy for CLV).

Import `ifood` data.

```{r}
ifood <- read_csv("ifood.csv")
```

Compute total spend variable.

```{r}
ifood <- ifood |>
  mutate(total_spend = MntWines + MntFruits + MntMeatProducts + MntFishProducts + MntSweetProducts + MntGoldProds)
```


```{r}

```


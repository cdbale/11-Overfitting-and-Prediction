---
title: "Overfitting and Prediction"
subtitle: "MCB Consulting Group (MKTG 411)"
output: 
  ioslides_presentation:
    widescreen: true
    css: style.css
---

## Marketing Analytics Process

<center>
![](Figures/process_model.png){width="900px"}
</center>

## Inferential Modeling Workflow

<center>
![](Figures/workflow-inference_preprocess-evaluate.png){width=900px}
</center>

---

![](Figures/hex_rsample-yardstick.png){width=850px}

## Review the Workflow

<center>
![](Figures/workflow-inference.png){width=900px}
</center>

## Model Comparison Review

Remember that we use a measure of *overall model fit* to compare how well different models fit the same data. But there's a problem.

## Models Can "Memorize" Data

- As we add explanatory variables to a model, it will *automatically* fit the data better.
- The more variables a model can access, the more likely it will use **random variation** in your $X$ variables to try to explain variation in your $Y$ variable.
- This occurs *even if your explanatory variables are uncorrelated with your dependent variable*.

Example: what if we were to build a model for the height of students in this class?

## Analogy: Students and Models

Imagine there are two students with different strategies for studying for a test.

- Student 1: takes the same practice test multiple times until they can get 100%.
- Student 2: studies class notes, homeworks assignments, etc. Takes the practice test once and gets an 80%.

Which student is **likely** to perform best on the actual test?

## Underfitting and Overfitting

Remember that we want a model that gives us a good representation of our story. The data we observe is just one outcome, or set of outcomes, from that story. As such, we don't want to learn too little *or* too much from our data.

This is called **underfitting** and **overfitting**, respectively, with the latter the problem we've just discussed.

<center>
![](Figures/underfit_overfit.png){width=800px}
</center>

## Can Our Existing Model Evaluation Metrics Help?

- $R$-squared: percent of variation in $Y$ explained by our $X$ variables
- Adjusted $R$-squared: same as above, but with a penalty for every $X$ variable. Adding $X$ variables that do not explain variation in $Y$ can lower the adjusted $R$-squared.

## $R$-squared measures cannot reliably identify over-fitting or under-fitting. Why?

<center>
![](Figures/underfit_overfit.png){width=800px}
</center>

## How Can We Use This to Help Our Clients?

Imagine Kroger has shared a new data set with us. Each observation in the data contains the monthly sales of products in the Ice Cream/Novelties product category at the *store level*, which is the target, or dependent variable. Kroger has already built a model for sales that includes many explanatory variables:

- category characteristics (number of products, average volumetric price, number of promotions/sales)
- store characteristics (number of in-store categories, whether the store uses a Category Captain)
- demographics of the customer base of the store (average income, age, household size)
- dummy variables for quarters and holidays

## Kroger has requested the following:

- Are there any other explanatory variables we believe would help explain sales in the Ice Cream/Novelties category?
- Can we prescribe a robust method for assessing whether their model is underfit or overfit to their data?

## Predictive Fit

Instead of measuring how well a model fits the *data*, we can measure how well a model *predicts*. Why should this help?

- A model can't overfit to data it hasn't seen.
- A model that is doing a better job of modeling the data generating process should be able to do a better job at predicting data generated by that same process.

## Import our Libraries and Data

```{r message=FALSE}
# Load packages.
library(tidyverse)
library(tidymodels)

# Import soup data.
soup_data <- read_csv("soup_data.csv") |> 
  select(Sales, Any_Disp_Spend, Any_Feat_Spend, Any_Price_Decr_Spend)
```

## For Illustrative Purposes ONLY!

We are adding random variables to our data to see how they affect our model. We DO NOT normally do this.

```{r}
set.seed(7)

soup_data <- soup_data |> 
  mutate(
    nonsense_01 = rnorm(nrow(soup_data), mean = 0, sd = 5),
    nonsense_02 = rnorm(nrow(soup_data), mean = 50, sd = 10),
    nonsense_03 = rnorm(nrow(soup_data), mean = -10, sd = 20),
    nonsense_04 = rnorm(nrow(soup_data), mean = 100, sd = 100),
    nonsense_05 = runif(nrow(soup_data), min = -5, max = 5),
    nonsense_06 = runif(nrow(soup_data), min = 25, max = 500),
    nonsense_07 = runif(nrow(soup_data), min = -50, max = 10),
    nonsense_08 = rbinom(nrow(soup_data), size = 1, prob = 0.25),
    nonsense_09 = rbinom(nrow(soup_data), size = 3, prob = 0.10),
    nonsense_10 = rbinom(nrow(soup_data), size = 7, prob = 0.85)
  )
```

## Data Splitting (We DO Normally Do This)

In order to compute predictive fit, we need to split the data into **training** data (used to fit the model) and **testing** data (used to compute predictive fit).

```{r}
# Set the randomization seed.
set.seed(42)

# Split the data.
soup_split <- initial_split(soup_data, prop = 0.90)

soup_split
```

`initial_split()` appends 'meta-information' for other functions to use (i.e., which observations are contained in the train and test data sets). Essentially, we are adding either a `Training` label or a `Testing` label to each observation in the original data.

---

We can access the training data with `training()`.

```{r}
# Access the training data.
training(soup_split)
```

---

We can access the testing data with `testing()`.

```{r}
# Access the testing data.
testing(soup_split)
```

## Fit with Training Data

We fit the model using only the *training data* to fit (i.e., train) the model.

```{r}
# Fit the simple model with training data.
fit_simple <- linear_reg() |> 
  set_engine("lm") |> 
  fit(
    Sales ~ Any_Disp_Spend + Any_Feat_Spend + Any_Price_Decr_Spend, 
    data = training(soup_split)
  )

# Fit the complex model with training data
fit_complex <- linear_reg() |> 
  set_engine("lm") |> 
  fit(
    Sales ~ ., 
    data = training(soup_split)
  )
```

## Assess Model Performance with R-Squared (Unreliable)

```{r eval=FALSE}
# Compare overall model fit.
bind_rows(
  glance(fit_simple),
  glance(fit_complex)
)
```

```{r echo=FALSE}
bind_rows(
  glance(fit_simple),
  glance(fit_complex)) |> 
  select(r.squared:logLik) |> 
  as.data.frame()
```

Which model is *technically* better based on $R$-squared?

## Predict with Testing Data (THIS IS THE WAY.)

Now that we have trained our models, we use `predict()` for model comparison where the `new_data` is the testing data. We compare the predicted outcome to the *actual* outcome and compute the **root mean squared error** (RMSE), a squared average of the difference between the truth and our estimate. Closer to 0 is better!

```{r}
# Compute predictive fit.
fit_simple |> 
  predict(new_data = testing(soup_split)) |>
  bind_cols(testing(soup_split)) |>
  rmse(truth = Sales, estimate = .pred)
```

---

```{r}
fit_complex |> 
  predict(new_data = testing(soup_split)) |>
  bind_cols(testing(soup_split)) |>
  rmse(truth = Sales, estimate = .pred)
```

According to the test data RMSE, which model gives the best fit?

## What About Training Data Predictions? (Common Mistake)

```{r}
# Compute predictive fit for the simple model on the training data (BAD)
fit_simple |> 
  predict(new_data = training(soup_split)) |>
  bind_cols(training(soup_split)) |>
  rmse(truth = Sales, estimate = .pred)
```

---

```{r}
# Compute predictive fit for the complex model on the training data (BAD)
fit_complex |> 
  predict(new_data = training(soup_split)) |>
  bind_cols(training(soup_split)) |>
  rmse(truth = Sales, estimate = .pred)
```

Based on training data predictions, which model is better/more accurate? Is this the expected result?

## Wait...What Am I Supposed to Do?

1. *Train* models on *Train* Data
2. *Choose* your best model after *Testing* it on *Test* Data

## Live Coding

`iFood` was happy with the work you completed on your projects. They have asked us to build a model for the total amount that a customer purchases (i.e., a proxy for CLV).

Import `ifood` data.

```{r}
ifood <- read_csv("ifood.csv")
```

Compute total spend variable.

```{r}
ifood <- ifood |>
  mutate(total_spend = MntWines + MntFruits + MntMeatProducts + MntFishProducts + MntSweetProducts + MntGoldProds)
```

---

Let's come up with a story, build a model according to that story, and train and assess that model appropriately. To ensure we are not overfitting, let's also construct a simpler model for comparison.

## Wrapping Up

*Summary*

- Discussed overfitting (and underfitting).
- Introduced splitting data.
- Argued that predictive fit can't be fooled as easily as $R$-squared

*Next Time*

- Completing the inferential workflow with preprocessing data.

*Supplementary Material*

- *Tidy Modeling with R* Chapter 9.1-9.2

## Exercise 10

Return to `soup_data` and the models you estimated for Exercise 9.

1. Split the data. Use `initial_time_split()` to place the first 90% of observations in the data in training data and the last 10% in testing data. Note that the data is already sorted after you import it, otherwise we would need to arrange it appropriately.
2. Fit the models from Exercise 9 on the training data.
3. Compute the RMSE for each model using the testing data.
4. Identify the best-fitting model based on $R^2$, Adjusted $R^2$, and RMSE. Is it the same? Why or why not?
5. Render the Quarto document into Word and upload to Canvas.

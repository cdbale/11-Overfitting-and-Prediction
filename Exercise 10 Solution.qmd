---
title: "Exercise 10"
author: "Cameron Bale"
format: docx
---

Return to `soup_data` and the models you estimated for Exercise 9.

1. Split the data. Use `initial_time_split()` to place the first 90% of observations in the data in training data and the last 10% in testing data. Note that the data is already sorted after you import it, otherwise we would need to arrange it appropriately.
2. Fit the models from Exercise 9 on the training data.
3. Compute the RMSE for each model using the testing data.
4. Identify the best-fitting model based on $R^2$, Adjusted $R^2$, and RMSE. Is it the same? Why or why not?
5. Render the Quarto document into Word and upload to Canvas.

**Five points total, one point for each step above.**

## Split the Data

Let's load the packages we'll need, import the data, and split the data as specified.

```{r}
# Load packages.
library(tidyverse)
library(tidymodels)

# Import and filter data.
soup_data <- read_csv("soup_data.csv") |> 
  filter(Retailer_Trade_Areas == "WEST CENSUS TA", Brand_High == "CAMPBELL'S")

# Split the data.
soup_split <- initial_time_split(soup_data, prop = 0.90)
```

## Fit the Models

Now let's refit the models we ran previously.

```{r}
# Full model.
fit_01 <- linear_reg() |> 
  set_engine(engine = "lm") |> 
  fit(
    Sales ~ Any_Disp_Spend + Any_Feat_Spend + Any_Price_Decr_Spend, 
    data = training(soup_split)
  )

# Model without display spend.
fit_02 <- linear_reg() |> 
  set_engine(engine = "lm") |> 
  fit(
    Sales ~ Any_Feat_Spend + Any_Price_Decr_Spend, 
    data = training(soup_split)
  )
```

## Overall Model Fit

Now let's compute and compare RMSE, as well as R-squared and the Adjusted R-squared.

```{r}
# Compute RMSE.
rmse_01 <- fit_01 |> 
  predict(new_data = testing(soup_split)) |>
  bind_cols(testing(soup_split)) |>
  rmse(truth = Sales, estimate = .pred)

rmse_02 <- fit_02 |> 
  predict(new_data = testing(soup_split)) |>
  bind_cols(testing(soup_split)) |>
  rmse(truth = Sales, estimate = .pred)

# Compare RMSEs.
tibble(model = c("Full model", "Model without display spend")) |> 
  bind_cols(
    bind_rows(
      rmse_01,
      rmse_02
    )
  ) |> 
  arrange(.estimate)
```

Based on RMSE, the best-fitting model is the model with all three explanatory variables -- the "full model."

```{r}
# Model comparison.
tibble(
  model = c(
    "Full model", 
    "Model without display spend"
  )
) |> 
  bind_cols(
    bind_rows(
      glance(fit_01), 
      glance(fit_02)
    )
  ) |> 
  arrange(desc(r.squared))
```

According to R-squared and Adjusted R-squared, the "full model" that includes all three of the explanatory variables fits best.

The fact that RMSE identifies the same best-fitting model as R-squared and Adjusted R-squared suggests that the "full model" isn't overfitting the data. That might not be surprising since the "full model" only has three explanatory variables. Managerially speaking, this suggests that spend in all three categories - feature, display, and price decreases, is meaningful in driving sales in the soup category.
